### GoLang Testing: Benchmarks

Benchmarking is used to verify performance of our code and is run by `go test` tool just like unit tests. Benchmark functions differ from test functions by having a `Benchmark` prefix in them. By default, no benchmarks are run when calling `go test` so they need to be called explicitly by specifying a `-bench` option. It worth noting that benchmark results may differ from time to time and is dependent on variety of factors such as the current CPU load at the time of the benchmark execution. So, the ideal situation to run benchmarks would be on a dedicated machine that doesn't have any background tasks, CPU thermo settings, etc. In order to mitigate some of the factors, the benchmark tool in Go will need to run the function under test enough time until the results become stable enough to judge about the execution time. Let's look at how to organize benchmark code so that it leads to the correct results.

```golang
func BenchmarkBubbleSort(b *testing.B) {
	data := randomData(10000)
	b.ResetTimer()
	for n := 0; n < b.N; n++ {
        BubbleSort(data)
    }
}
```

This shows a simple case where the function is benchmarked for an array of 10000 elements. As you noticed, we call our function `BubbleSort` in a for loop that depends on the number provided by the benchmark caller which would increment it until it makes sure the result is stable enough to make judgment about the execution time. 

```bash
$ go test -v -bench=.
goos: darwin
goarch: amd64
BenchmarkBubbleSort-12    	      30	  41658001 ns/op
PASS
ok  	_/bubblesort	1.890s
```

The output shows us that the benchmark was called 30 times until it reached to a point where it could be possible to make a decision about the execution time which was `41658001 ns` per loop. Since the input is generated by `randomData()` as part of benchmark, it may affect the result so we call `b.ResetTimer()` to start the benchmark measuring right before the actual `BubbleSort` execution.

It may be hard to judge on the function performance based just on that one input parameter. So, we might want to run it with the input arrays of different sizes. If we do it in a regular for loop just like we do with unit tests, the result will be incorrect since the benchmark function will be measured as a whole rather than on per test case basis. In order to get correct results, the recommendation before Go 1.9 was to write a new benchmark function which will call some shared code with a input parameter for the function under test. With the introduction of sub-test and sub-benchmarks, it became much simpler to do that.

```golang
func BenchmarkBubbleSort(b *testing.B) {
	tests := []int{ 10000, 20000, 30000, 40000, 50000 }
	for _, test := range tests {
		b.Run(strconv.Itoa(test), func(pb *testing.B) {
			data := randomData(test)
			b.ResetTimer()
			for n := 0; n < b.N; n++ {
				BubbleSort(data)
			}
		})
	}
}
```

Thanks to `Run()` method, it became quite simple to separate sub-benchmarks and correctly measure the performance.

```bash
$ go test -v -bench=.
goos: darwin
goarch: amd64
BenchmarkBubbleSort/10000-12         	2000000000	         0.06 ns/op
BenchmarkBubbleSort/20000-12         	2000000000	         0.26 ns/op
BenchmarkBubbleSort/30000-12         	       1	1149653432 ns/op
BenchmarkBubbleSort/40000-12         	       1	2046792398 ns/op
BenchmarkBubbleSort/50000-12         	       1	3191425493 ns/op
PASS
ok  	_/bubblesort	18.045s
```